{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af524363",
   "metadata": {},
   "source": [
    "# Train and deploy a Faster R-CNN Object Detection model\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/unionai-oss/faster-rcnn-object-detection-computer-vision-train-and-deploy/blob/main/tutorial.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78c9dea",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7e6ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !git clone https://github.com/unionai-oss/faster-rcnn-object-detection-computer-vision-train-and-deploy\n",
    "    %cd faster-rcnn-object-detection-computer-vision-train-and-deploy\n",
    "    !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24988b74",
   "metadata": {},
   "source": [
    "### üîê Authenticate\n",
    "To use **Union.ai**, you'll need to authenticate your account. Follow the appropriate step based on your setup:  \n",
    "\n",
    "##### üî∏ **Using Union BYOC Enterprise**  \n",
    "\n",
    "If you're using a **[Union BYOC Enterprise](https://www.union.ai/pricing)** account, log in with the following command:  \n",
    "```bash\n",
    "union create login --host <union-host-url>\n",
    "```\n",
    "\n",
    "Replace <union-host-url> with your organization's Union instance URL.\n",
    "\n",
    "##### üî∏ Using Union Serverless\n",
    "If you're using [Union Serverless](https://www.union.ai/) , authenticate by running the command below:\n",
    "\n",
    "Create an account for free at [Union.ai](https://union.ai) if you don't have one yet:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b121e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üåü Authenticate to union serverless\n",
    "!union create login --serverless --auth device-flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bb871b",
   "metadata": {},
   "source": [
    "## Training Faster RCNN Object Detetection Model Pipeline\n",
    "\n",
    "Run the command below to train a Faster RCNN Object Detection model using the Union.ai CLI. This command will create a new pipeline and start the training process.\n",
    "\n",
    "The first time you this command it will take a while to download the model and set up the environment. The subsequent runs will be faster as the model will be cached.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a93053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this command to start the training workflow & container building\n",
    "!union run --remote workflows/train-frcnn-pipeline.py faster_rcnn_train_workflow --epochs 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80691c4b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef371fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile workflow.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c102c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# containers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd18f9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirmets.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8900e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bd481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9f0c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f7e7ef0",
   "metadata": {},
   "source": [
    "## Run Model Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a1aa63",
   "metadata": {},
   "source": [
    " lets pull donw our same datset locally to run the model on examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d440fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets call our downlaod dataset from earlier locally\n",
    "!union run tasks/data.py download_hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001628eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ee211033",
   "metadata": {},
   "source": [
    "We will pull down the latest Model Artifact from Union and save it locally. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc90613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from union import Artifact, UnionRemote\n",
    "from flytekit.types.file import FlyteFile\n",
    "import torch\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Download & save the fine-tuned model from Union Artifacts\n",
    "# --------------------------------------------------\n",
    "FRCCNFineTunedModel = Artifact(name=\"frccn_fine_tuned_model\")\n",
    "\n",
    "query = FRCCNFineTunedModel.query(\n",
    "    project=\"default\",\n",
    "    domain=\"development\",\n",
    "    # version=\"anmrqcq8pfbnlp42j2vp/n3/0/o0\"  # Optional: specify version. Will download the latest version if not specified\n",
    ")\n",
    "remote = UnionRemote()\n",
    "artifact = remote.get_artifact(query=query)\n",
    "model_file: FlyteFile = artifact.get(as_type=FlyteFile)\n",
    "model = torch.load(model_file.download(), map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4979977",
   "metadata": {},
   "source": [
    "Let's run the model locally on an example image and draw the bounding boxes on the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84040cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "from flytekit.types.file import FlyteFile\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from torchvision.transforms import functional as F\n",
    "from union import Artifact, UnionRemote\n",
    "from io import BytesIO\n",
    "\n",
    "# Define labels map\n",
    "labels_map = {1: \"Union Sticker\", 2: \"Flyte Sticker\"}\n",
    "\n",
    "# Check and set the available device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "font_url = \"https://github.com/google/fonts/raw/refs/heads/main/apache/ultra/Ultra-Regular.ttf\"\n",
    "response = requests.get(font_url)\n",
    "font = ImageFont.truetype(BytesIO(response.content), size=20)\n",
    "\n",
    "\n",
    "# Function to draw bounding boxes\n",
    "def draw_boxes(image, boxes, labels, scores, labels_map):\n",
    "    draw = ImageDraw.Draw(image, 'RGBA')\n",
    "    # font = ImageFont.truetype(urlopen(truetype_url), size=20)\n",
    "    # font = ImageFont.load_default() # default font in pil\n",
    "\n",
    "\n",
    "    colors = {\n",
    "        0: (255, 173, 10, 200),  # Class 0 color (e.g., blue)\n",
    "        1: (28, 140, 252, 200),  # Class 1 color (e.g., orange)\n",
    "    }\n",
    "    colors_fill = {\n",
    "        0: (255, 173, 10, 100),  # Class 0 fill color (e.g., bluea)\n",
    "        1: (28, 140, 252, 100),  # Class 1 fill color (e.g., orangea)\n",
    "    }\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score > 0.6: # adjust threshold as needed\n",
    "          color = colors.get(label, (0, 255, 0, 200))\n",
    "          fill_color = colors_fill.get(label, (0, 255, 0, 100))\n",
    "          draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline=color, width=3)\n",
    "          draw.rectangle([(box[0], box[1]), (box[2], box[3])], fill=fill_color)\n",
    "          label_text = f\"{labels_map[label]}: {score:.2f}\"\n",
    "          text_size = font.getbbox(label_text)\n",
    "          draw.rectangle([(box[0], box[1] - text_size[1]), (box[0] + text_size[0], box[1])], fill=color)\n",
    "          draw.text((box[0], box[1] - text_size[1]), label_text, fill=\"white\", font=font)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "# Load a single test image\n",
    "image_path = '/content/faster-rcnn-object-detection-computer-vision-train-and-deploy/dataset/swag/images/1bd5a6b5-20240916_133544.jpg'\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "image_tensor = F.to_tensor(image).unsqueeze(0).to(device)\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(image_tensor)\n",
    "\n",
    "# Get the boxes, labels, and scores\n",
    "boxes = outputs[0]['boxes'].cpu().numpy()\n",
    "labels = outputs[0]['labels'].cpu().numpy()\n",
    "scores = outputs[0]['scores'].cpu().numpy()\n",
    "\n",
    "# Define labels map\n",
    "labels_map = {0: \"Background\", 1: \"Union Sticker\", 2: \"Flyte Sticker\"}\n",
    "\n",
    "# Draw the boxes on the image\n",
    "image_with_boxes = draw_boxes(image, boxes, labels, scores, labels_map)\n",
    "\n",
    "# Display the image\n",
    "image_with_boxes.show()\n",
    "\n",
    "# Save the image\n",
    "image_with_boxes.save('output_image.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c90f64",
   "metadata": {},
   "source": [
    "We can use the same draw bounding boxes function to loop over the frame in a video.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ac2bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "from flytekit.types.file import FlyteFile\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from torchvision.transforms import functional as F\n",
    "from union import Artifact, UnionRemote\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "# ------------------------------------\n",
    "# create video writer\n",
    "# ------------------------------------\n",
    "\n",
    "# Video path and properties\n",
    "video_path = \"dataset/swag/videos/union_sticker_video.mp4\"\n",
    "video = cv2.VideoCapture(video_path)\n",
    "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frames_per_second = video.get(cv2.CAP_PROP_FPS)\n",
    "num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "\n",
    "# Initialize video writer\n",
    "video_writer = cv2.VideoWriter(\n",
    "    \"object_detection_video.mp4\",\n",
    "    cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
    "    fps=float(frames_per_second),\n",
    "    frameSize=(width, height),\n",
    "    isColor=True,\n",
    ")\n",
    "\n",
    "\n",
    "def run_inference_video(video, model, device, labels_map):\n",
    "    while True:\n",
    "        hasFrame, frame = video.read()\n",
    "        if not hasFrame:\n",
    "            break\n",
    "\n",
    "        # Convert frame to PIL image\n",
    "        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        image_tensor = F.to_tensor(image).unsqueeze(0).to(device)\n",
    "\n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(image_tensor)\n",
    "\n",
    "        # Get the boxes, labels, and scores\n",
    "        boxes = outputs[0][\"boxes\"].cpu().numpy()\n",
    "        labels = outputs[0][\"labels\"].cpu().numpy()\n",
    "        scores = outputs[0][\"scores\"].cpu().numpy()\n",
    "\n",
    "        # Draw the boxes on the image\n",
    "        image_with_boxes = draw_boxes(image, boxes, labels, scores, labels_map)\n",
    "\n",
    "        # Convert back to OpenCV image format\n",
    "        result_frame = cv2.cvtColor(np.array(image_with_boxes), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        yield result_frame\n",
    "\n",
    "\n",
    "# Run inference and write video\n",
    "for frame in run_inference_video(video, model, device, labels_map):\n",
    "    video_writer.write(frame)\n",
    "\n",
    "# Release resources\n",
    "video.release()\n",
    "video_writer.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200eb455",
   "metadata": {},
   "source": [
    "This example below shows how you could run the model on a live video feed. This won't run in the notebook, but you can run it in your local environment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8746ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS WON\"T WORK IN COLAB\n",
    "# Webcam example \n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import time\n",
    "from torchvision.transforms import functional as F\n",
    "from huggingface_hub import hf_hub_download\n",
    "from union import Artifact, UnionRemote\n",
    "from flytekit.types.file import FlyteFile\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Load the fine-tuned SSD model from Union Artifact\n",
    "# --------------------------------------------------\n",
    "FRCCNFineTunedModel = Artifact(name=\"frccn_fine_tuned_model\")\n",
    "query = FRCCNFineTunedModel.query(\n",
    "    project=\"default\",\n",
    "    domain=\"development\",\n",
    "    # version=\"anmrqcq8pfbnlp42j2vp/n3/0/o0\"  # Optional: specify version\n",
    ")\n",
    "remote = UnionRemote()\n",
    "artifact = remote.get_artifact(query=query)\n",
    "model_file: FlyteFile = artifact.get(as_type=FlyteFile)\n",
    "model = torch.load(model_file.download(), map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Function to process a single frame and draw bounding boxes\n",
    "# --------------------------------------------------\n",
    "def process_frame(frame):\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image_tensor = F.to_tensor(frame_rgb).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(image_tensor)\n",
    "\n",
    "    boxes = prediction[0]['boxes'].cpu().numpy()\n",
    "    scores = prediction[0]['scores'].cpu().numpy()\n",
    "    labels = prediction[0]['labels'].cpu().numpy()\n",
    "\n",
    "    for i, box in enumerate(boxes):\n",
    "        if scores[i] > 0.5: # Confidence threshold for detection \n",
    "            x_min, y_min, x_max, y_max = box\n",
    "            cv2.rectangle(frame, (int(x_min), int(y_min)), (int(x_max), int(y_max)), (0, 255, 0), 2)\n",
    "            label = f\"Class {labels[i]}: {scores[i]:.2f}\"\n",
    "            cv2.putText(frame, label, (int(x_min), int(y_min) - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    return frame\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Run feed with frame skipping option for efficiency\n",
    "# --------------------------------------------------\n",
    "def run_video_feed(skip_frames=5):\n",
    "    frame_skip = skip_frames\n",
    "    frame_count = 0\n",
    "    last_processed_frame = None\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video stream.\")\n",
    "        return\n",
    "\n",
    "    prev_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Failed to capture frame.\")\n",
    "            break\n",
    "\n",
    "        current_time = time.time()\n",
    "        fps = 1 / (current_time - prev_time)\n",
    "        prev_time = current_time\n",
    "\n",
    "        if frame_count % frame_skip == 0:\n",
    "            last_processed_frame = process_frame(frame)\n",
    "            fps_text = f\"FPS: {fps:.2f}\"\n",
    "            cv2.putText(last_processed_frame, fps_text, (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "\n",
    "        if last_processed_frame is not None:\n",
    "            cv2.imshow('Object Detection RCNN', last_processed_frame)\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Run the video feed function\n",
    "if __name__ == \"__main__\":\n",
    "    run_video_feed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a72e519",
   "metadata": {},
   "source": [
    "## Serving as an App on Union\n",
    "\n",
    "We can serve our model as an app on Union. This allows us to run the model in a production environment and make it available for use by other applications or users.\n",
    "\n",
    "This example will use Gradio, but we could also use any other web framework like Flask or FastAPI to serve our model as an API. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdf5e5e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
