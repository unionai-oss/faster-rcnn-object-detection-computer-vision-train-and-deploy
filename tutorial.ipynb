{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af524363",
   "metadata": {},
   "source": [
    "# Train and deploy a Faster R-CNN Object Detection model\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/unionai-oss/faster-rcnn-object-detection-computer-vision-train-and-deploy/blob/main/tutorial.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78c9dea",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7e6ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    !git clone https://github.com/unionai-oss/faster-rcnn-object-detection-computer-vision-train-and-deploy\n",
    "    %cd faster-rcnn-object-detection-computer-vision-train-and-deploy\n",
    "    !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24988b74",
   "metadata": {},
   "source": [
    "### üîê Authenticate\n",
    "To use **Union.ai**, you'll need to authenticate your account. Follow the appropriate step based on your setup:  \n",
    "\n",
    "##### üî∏ **Using Union BYOC Enterprise**  \n",
    "\n",
    "If you're using a **[Union BYOC Enterprise](https://www.union.ai/pricing)** account, log in with the following command:  \n",
    "```bash\n",
    "union create login --host <union-host-url>\n",
    "```\n",
    "\n",
    "Replace <union-host-url> with your organization's Union instance URL.\n",
    "\n",
    "##### üî∏ Using Union Serverless\n",
    "If you're using [Union Serverless](https://www.union.ai/) , authenticate by running the command below in the code cell:  \n",
    "\n",
    "Create an account for free at [Union.ai](https://union.ai) if you don't have one yet:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b121e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üåü Authenticate to union serverless\n",
    "!union create login --serverless --auth device-flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bb871b",
   "metadata": {},
   "source": [
    "## Training Faster RCNN Object Detetection Model Pipeline\n",
    "\n",
    "Run the command below to train a Faster RCNN Object Detection model using the Union.ai CLI. This command will create a new pipeline and start the training process.\n",
    "\n",
    "The first time you this command it will take a while to download the model and set up the environment.\n",
    "\n",
    "The subsequent runs will be faster as the container, model, and data will be cached.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a93053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üëá Run this command to start the training workflow & container building\n",
    "!union run --remote workflows/train-frcnn-pipeline.py faster_rcnn_train_workflow --epochs 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5485648",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9901866d",
   "metadata": {},
   "source": [
    "### üîé Explore the Code  \n",
    "\n",
    "- The command above is using files from the [`workflows/`](workflows/train-frcnn-pipeline.py) and [`tasks`](tasks/) folders that got cloned on setup.\n",
    "\n",
    "- The codeis added to this notebook for reference with the `%%writefile` magic command to overwrite the files if you want to make changes.\n",
    "\n",
    "- You do not need to run the code cells with `%%writefile` unless you want to make changes to the pipeline or tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80691c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile workflows/train-frcnn-pipeline.py\n",
    "\n",
    "from union import workflow\n",
    "\n",
    "from tasks.data import download_hf_dataset, verify_data_and_annotations\n",
    "from tasks.model import download_model, evaluate_model, train_model, upload_model_to_hub\n",
    "\n",
    "\n",
    "# %% ------------------------------\n",
    "# Object Detection Workflow\n",
    "# --------------------------------\n",
    "@workflow\n",
    "def faster_rcnn_train_workflow(\n",
    "    epochs: int = 3, classes: int = 3, hf_repo_id: str = \"\"\n",
    ") -> None:\n",
    "\n",
    "    dataset_dir = download_hf_dataset(\n",
    "        repo_id=\"sagecodes/union_flyte_swag_object_detection\"\n",
    "    )\n",
    "    model_file = download_model()\n",
    "    verify_data_and_annotations(dataset_dir=dataset_dir)\n",
    "    trained_model = train_model(\n",
    "        model_file=model_file,\n",
    "        dataset_dir=dataset_dir,\n",
    "        num_epochs=epochs,\n",
    "        num_classes=classes,\n",
    "    )\n",
    "    evaluate_model(model=trained_model, dataset_dir=dataset_dir)\n",
    "    # upload_model_to_hub(model=trained_model, repo_name=hf_repo_id) # uncomment to upload the model to Hugging Face Hub\n",
    "\n",
    "\n",
    "# union run --remote workflows/train-frcnn-pipeline.py faster_rcnn_train_workflow --epochs 3\n",
    "# union run --remote workflows/train-frcnn-pipeline.py faster_rcnn_train_workflow --epochs 3 --hf_repo_id \"sagecodes/cv-object-rcnn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c102c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile containers.py\n",
    "from flytekit import ImageSpec, Resources\n",
    "# from union.actor import ActorEnvironment\n",
    "\n",
    "container_image = ImageSpec(\n",
    "     name=\"fine-tune-qlora\",\n",
    "    requirements=\"requirements.txt\",\n",
    "    pip_extra_index_url=[\"https://download.pytorch.org/whl/cu118\"],  #enables +cu118 builds\n",
    "    builder=\"union\",\n",
    "    cuda=\"11.8\",  # ensure GPU + CUDA layer is available\n",
    "    apt_packages=[\"gcc\", \"g++\"],  # optional, for packages like quantization \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd18f9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "union==0.1.181\n",
    "flytekit==1.15.4\n",
    "torch==2.5.1\n",
    "torchvision==0.20.1\n",
    "matplotlib==3.10.3\n",
    "pycocotools==2.0.8\n",
    "datasets==2.14.4\n",
    "huggingface_hub\n",
    "python-dotenv==1.1.0\n",
    "opencv-python==4.11.0.86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8900e81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tasks/data.py\n",
    "\n",
    "\"\"\"\n",
    "this module contains the data loading and preprocessing functions\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "from textwrap import dedent\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from flytekit import Deck, Resources, current_context, task\n",
    "from flytekit.types.directory import FlyteDirectory\n",
    "from flytekit.types.file import FlyteFile\n",
    "\n",
    "from containers import container_image\n",
    "from tasks.helpers import dataset_dataloader, image_to_base64\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "# %% ------------------------------\n",
    "# Download dataset - task\n",
    "# --------------------------------\n",
    "@task(\n",
    "    container_image=container_image,\n",
    "    enable_deck=True,\n",
    "    cache=True,\n",
    "    cache_version=\"1.333\",\n",
    "    requests=Resources(cpu=\"2\", mem=\"2Gi\"),\n",
    ")\n",
    "def download_hf_dataset(\n",
    "    repo_id: str = \"sagecodes/union_swag_coco\",\n",
    "    local_dir: str = \"dataset\",\n",
    "    sub_folder: str = \"swag\",\n",
    ") -> FlyteDirectory:\n",
    "\n",
    "    from huggingface_hub import snapshot_download\n",
    "\n",
    "    if local_dir:\n",
    "        dataset_dir = os.path.join(local_dir)\n",
    "        os.makedirs(dataset_dir, exist_ok=True)\n",
    "\n",
    "    # Download the dataset repository\n",
    "    repo_path = snapshot_download(\n",
    "        repo_id=repo_id, repo_type=\"dataset\", local_dir=local_dir\n",
    "    )\n",
    "    if sub_folder:\n",
    "        repo_path = os.path.join(repo_path, sub_folder)\n",
    "        # use sub_folder to return a specific folder from the dataset\n",
    "\n",
    "    print(f\"Dataset downloaded to {repo_path}\")\n",
    "\n",
    "    print(f\"Files in dataset directory: {os.listdir(repo_path)}\")\n",
    "\n",
    "    return FlyteDirectory(repo_path)\n",
    "\n",
    "\n",
    "# %% ------------------------------\n",
    "# visualize data - task\n",
    "# --------------------------------\n",
    "@task(\n",
    "    container_image=container_image,\n",
    "    enable_deck=True,\n",
    "    requests=Resources(cpu=\"2\", mem=\"4Gi\"),\n",
    ")\n",
    "def verify_data_and_annotations(dataset_dir: FlyteDirectory) -> FlyteFile:\n",
    "\n",
    "    import matplotlib.patches as patches\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Download the dataset locally from the FlyteDirectory\n",
    "    dataset_dir.download()\n",
    "    local_dataset_dir = dataset_dir.path\n",
    "\n",
    "    # Load the dataset\n",
    "    data_loader = dataset_dataloader(\n",
    "        root=local_dataset_dir, annFile=\"train.json\", shuffle=True\n",
    "    )\n",
    "\n",
    "    # Number of images to display\n",
    "    num_images = 9\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))  # Create a 3x3 grid\n",
    "    axes = axes.flatten()  # Flatten the axes array for easier iteration\n",
    "\n",
    "    images_plotted = 0  # Counter for images plotted\n",
    "\n",
    "    # Plot images along with annotations\n",
    "    for batch_idx, (images, targets) in enumerate(data_loader):\n",
    "        for i, image in enumerate(images):\n",
    "            if images_plotted >= num_images:\n",
    "                break  # Limit to 9 images\n",
    "\n",
    "            # Plot the image\n",
    "            img = image.cpu().permute(\n",
    "                1, 2, 0\n",
    "            )  # Convert image to HWC format for plotting\n",
    "            ax = axes[images_plotted]  # Access the correct subplot\n",
    "            ax.imshow(img)\n",
    "\n",
    "            # Iterate over the list of annotations (objects) for the current image\n",
    "            for annotation in targets[i]:\n",
    "                # Extract the bounding box\n",
    "                bbox = annotation[\n",
    "                    \"bbox\"\n",
    "                ]  # This is in [x_min, y_min, width, height] format\n",
    "\n",
    "                # Convert [x_min, y_min, width, height] to [x_min, y_min, x_max, y_max]\n",
    "                x_min, y_min, width, height = bbox\n",
    "                x_max = x_min + width\n",
    "                y_max = y_min + height\n",
    "\n",
    "                # Draw the bounding box\n",
    "                rect = patches.Rectangle(\n",
    "                    (x_min, y_min),\n",
    "                    width,\n",
    "                    height,\n",
    "                    linewidth=2,\n",
    "                    edgecolor=\"r\",\n",
    "                    facecolor=\"none\",\n",
    "                )\n",
    "                ax.add_patch(rect)\n",
    "\n",
    "            # Increment image counter\n",
    "            images_plotted += 1\n",
    "\n",
    "        if images_plotted >= num_images:\n",
    "            break  # Stop if we've plotted the desired number of images\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the grid of images and annotations\n",
    "    output_img = \"data_verification_grid.png\"\n",
    "    plt.savefig(output_img)\n",
    "    plt.close()\n",
    "\n",
    "    # Convert the image to base64 for display in FlyteDeck\n",
    "    verification_image_base64 = image_to_base64(output_img)\n",
    "\n",
    "    # Display the results in FlyteDeck\n",
    "    ctx = current_context()\n",
    "    deck = Deck(\"Data Verification\")\n",
    "    html_report = dedent(\n",
    "        f\"\"\"\n",
    "    <div style=\"font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "       <h2 style=\"color: #2C3E50;\">Data Verification: Images and Annotations</h2>\n",
    "        <img src=\"data:image/png;base64,{verification_image_base64}\" width=\"600\">\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    # Append the HTML content to the deck\n",
    "    deck.append(html_report)\n",
    "    ctx.decks.insert(0, deck)\n",
    "\n",
    "    # Return the image file for further use in the workflow\n",
    "    return FlyteFile(output_img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bd481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile tasks/model.py\n",
    "\n",
    "# %%\n",
    "import base64\n",
    "import os\n",
    "from textwrap import dedent\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from dotenv import load_dotenv\n",
    "from flytekit import Deck, Resources, Secret, current_context, task\n",
    "from flytekit.types.directory import FlyteDirectory\n",
    "from flytekit.types.file import FlyteFile\n",
    "from torchvision.models.detection.faster_rcnn import (\n",
    "    FasterRCNN_MobileNet_V3_Large_320_FPN_Weights,\n",
    ")\n",
    "from torchvision.ops import box_iou\n",
    "from torchvision.transforms import transforms as T\n",
    "from typing_extensions import Annotated\n",
    "from union import Artifact\n",
    "\n",
    "from containers import container_image\n",
    "from tasks.helpers import dataset_dataloader, image_to_base64\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Define Artifacts\n",
    "FRCCNPreTrainedModel = Artifact(name=\"frccn_pretrained_model\")\n",
    "FRCCNFineTunedModel = Artifact(name=\"frccn_fine_tuned_model\")\n",
    "\n",
    "\n",
    "# %% ------------------------------\n",
    "# donwload model - task\n",
    "# --------------------------------\n",
    "@task(\n",
    "    container_image=container_image,\n",
    "    cache=True,\n",
    "    cache_version=\"1.334\",\n",
    "    requests=Resources(cpu=\"2\", mem=\"2Gi\"),\n",
    ")\n",
    "def download_model() -> Annotated[FlyteFile, FRCCNPreTrainedModel]:\n",
    "\n",
    "    model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(\n",
    "        weights=FasterRCNN_MobileNet_V3_Large_320_FPN_Weights, weights_only=True\n",
    "    )\n",
    "\n",
    "    save_dir = \"frccn_mobilenet_pretrained_model.pth\"\n",
    "    torch.save(model, save_dir)\n",
    "\n",
    "    # return model\n",
    "    return FRCCNPreTrainedModel.create_from(save_dir)\n",
    "\n",
    "\n",
    "# %% ------------------------------\n",
    "# train model - task\n",
    "# --------------------------------\n",
    "@task(\n",
    "    container_image=container_image,\n",
    "    enable_deck=True,\n",
    "    requests=Resources(cpu=\"2\", mem=\"8Gi\", gpu=\"1\"),\n",
    ")\n",
    "def train_model(\n",
    "    model_file: FlyteFile,\n",
    "    dataset_dir: FlyteDirectory,\n",
    "    num_epochs: int,\n",
    "    num_classes: int = 2,\n",
    "    conf_thresh: float = 0.75,\n",
    "    validate_every_n_epochs: int = 1,\n",
    ") -> Annotated[FlyteFile, FRCCNFineTunedModel]:\n",
    "\n",
    "    num_classes = num_classes + 1  # + 1 background)\n",
    "    print(f\"Using confidence threshold: {conf_thresh} for evaluation\")\n",
    "\n",
    "    num_epochs = num_epochs\n",
    "    best_mean_iou = 0\n",
    "    model_dir = \"models\"\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    dataset_dir.download()\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    local_dataset_dir = dataset_dir.path  # Use the local path for FlyteDirectory\n",
    "    data_loader = dataset_dataloader(root=local_dataset_dir, annFile=\"train.json\")\n",
    "    test_data_loader = dataset_dataloader(root=local_dataset_dir, annFile=\"train.json\")\n",
    "\n",
    "    # Load pretrained model\n",
    "    model = torch.load(model_file, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "    # Modify the model to add a new classification head based on the number of classes\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = (\n",
    "        torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
    "            in_features, num_classes\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Define optimizer and learning rate\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "    def evaluate_model(model, data_loader):\n",
    "        model.eval()\n",
    "        iou_list = []\n",
    "        correct_predictions, total_predictions = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, targets in data_loader:\n",
    "                images = [img.to(device) for img in images]\n",
    "                targets = [\n",
    "                    {\n",
    "                        \"boxes\": torch.tensor(\n",
    "                            [obj[\"bbox\"] for obj in t], dtype=torch.float32\n",
    "                        ).to(device),\n",
    "                        \"labels\": torch.tensor(\n",
    "                            [obj[\"category_id\"] for obj in t], dtype=torch.int64\n",
    "                        ).to(device),\n",
    "                    }\n",
    "                    for t in targets\n",
    "                ]\n",
    "                for t in targets:\n",
    "                    boxes = t[\"boxes\"]\n",
    "                    boxes[:, 2] += boxes[:, 0]\n",
    "                    boxes[:, 3] += boxes[:, 1]\n",
    "                    t[\"boxes\"] = boxes\n",
    "\n",
    "                outputs = model(images)\n",
    "\n",
    "                for i, output in enumerate(outputs):\n",
    "                    if \"scores\" not in output:\n",
    "                        continue\n",
    "\n",
    "                    keep = output[\"scores\"] > conf_thresh\n",
    "                    pred_boxes = output[\"boxes\"][keep]\n",
    "                    pred_labels = output[\"labels\"][keep]\n",
    "                    true_boxes = targets[i][\"boxes\"]\n",
    "                    true_labels = targets[i][\"labels\"]\n",
    "\n",
    "                    if pred_boxes.size(0) == 0 or true_boxes.size(0) == 0:\n",
    "                        continue\n",
    "\n",
    "                    iou = box_iou(pred_boxes, true_boxes)\n",
    "                    iou_list.append(iou.max(dim=1)[0].mean().item())  # best-match IoU\n",
    "\n",
    "                    # Accuracy: match predictions to true labels using best IoU\n",
    "                    max_iou_indices = iou.argmax(dim=1)\n",
    "                    matched_true_labels = true_labels[max_iou_indices]\n",
    "                    correct_predictions += (\n",
    "                        (pred_labels == matched_true_labels).sum().item()\n",
    "                    )\n",
    "                    total_predictions += len(pred_labels)\n",
    "\n",
    "        mean_iou = sum(iou_list) / len(iou_list) if iou_list else 0\n",
    "        accuracy = correct_predictions / total_predictions if total_predictions else 0\n",
    "        print(f\"Mean IoU: {mean_iou:.4f}, Accuracy: {accuracy:.4f}\", flush=True)\n",
    "\n",
    "        # TODO: save the model if mean_iou > best_mean_iou or add early stopping\n",
    "\n",
    "        return mean_iou, accuracy\n",
    "\n",
    "    epoch_logs = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for i, (images, targets) in enumerate(data_loader):\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [\n",
    "                {\n",
    "                    \"boxes\": torch.tensor(\n",
    "                        [obj[\"bbox\"] for obj in t], dtype=torch.float32\n",
    "                    ).to(device),\n",
    "                    \"labels\": torch.tensor(\n",
    "                        [obj[\"category_id\"] for obj in t], dtype=torch.int64\n",
    "                    ).to(device),\n",
    "                }\n",
    "                for t in targets\n",
    "            ]\n",
    "            for target in targets:\n",
    "                boxes = target[\"boxes\"]\n",
    "                boxes[:, 2] += boxes[:, 0]\n",
    "                boxes[:, 3] += boxes[:, 1]\n",
    "                target[\"boxes\"] = boxes\n",
    "\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += losses.item()\n",
    "\n",
    "            if i % 1 == 0:\n",
    "                print(\n",
    "                    f\"Epoch [{epoch}/{num_epochs}], Step [{i}/{len(data_loader)}], Loss: {losses.item():.4f}\",\n",
    "                    flush=True,\n",
    "                )\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        mean_iou, accuracy = evaluate_model(model, test_data_loader)\n",
    "        avg_train_loss = total_loss / len(data_loader)\n",
    "\n",
    "        epoch_logs.append(\n",
    "            {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": avg_train_loss,\n",
    "                \"val_accuracy\": accuracy,\n",
    "                \"val_mean_iou\": mean_iou,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if mean_iou > best_mean_iou:\n",
    "            best_mean_iou = mean_iou\n",
    "            torch.save(model.state_dict(), os.path.join(model_dir, \"best_model.pth\"))\n",
    "            print(\"Best model saved\")\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "    model_path = os.path.join(local_dataset_dir, \"frccn_finetuned_model.pth\")\n",
    "    # torch.save(model.state_dict(), model_path)\n",
    "    torch.save(model, model_path)\n",
    "\n",
    "    df = pd.DataFrame(epoch_logs)\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(df[\"epoch\"], df[\"train_loss\"], label=\"Train Loss\", marker=\"o\")\n",
    "    ax.plot(df[\"epoch\"], df[\"val_accuracy\"], label=\"Val Accuracy\", marker=\"s\")\n",
    "    ax.plot(df[\"epoch\"], df[\"val_mean_iou\"], label=\"Val Mean IoU\", marker=\"^\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Metric\")\n",
    "    ax.set_title(\"Training Metrics\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    plot_path = os.path.join(model_dir, \"training_metrics.png\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(plot_path)\n",
    "    plt.close()\n",
    "\n",
    "    # Convert to base64\n",
    "    def image_to_base64(img_path):\n",
    "        with open(img_path, \"rb\") as f:\n",
    "            return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "    plot_base64 = image_to_base64(plot_path)\n",
    "    deck = Deck(\"Training Metrics\")\n",
    "    deck.append(\n",
    "        f\"\"\"\n",
    "    <h2>Training Progress</h2>\n",
    "    <img src=\"data:image/png;base64,{plot_base64}\" width=\"600\"/>\n",
    "    <h3>Last Epoch:</h3>\n",
    "    <pre>{df.tail(1).to_string(index=False)}</pre>\n",
    "    \"\"\"\n",
    "    )\n",
    "    current_context().decks.insert(0, deck)\n",
    "\n",
    "    # return model\n",
    "    return FRCCNFineTunedModel.create_from(model_path)\n",
    "\n",
    "\n",
    "# %% ------------------------------\n",
    "# evaluate model - task\n",
    "# --------------------------------\n",
    "\n",
    "\n",
    "@task(\n",
    "    container_image=container_image,\n",
    "    enable_deck=True,\n",
    "    requests=Resources(cpu=\"2\", mem=\"8Gi\", gpu=\"1\"),\n",
    ")\n",
    "def evaluate_model(\n",
    "    model: torch.nn.Module, dataset_dir: FlyteDirectory, threshold: float = 0.75\n",
    ") -> str:\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    dataset_dir.download()\n",
    "    local_dataset_dir = dataset_dir.path\n",
    "    data_loader = dataset_dataloader(\n",
    "        root=local_dataset_dir, annFile=\"train.json\", shuffle=False\n",
    "    )\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    num_images = 9  # Number of images to display in the grid\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    iou_list, report = [], []\n",
    "    correct_predictions, total_predictions = 0, 0\n",
    "    images_plotted = 0\n",
    "    global_image_index = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, targets) in enumerate(data_loader):\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [\n",
    "                {\n",
    "                    \"boxes\": torch.tensor(\n",
    "                        [obj[\"bbox\"] for obj in t], dtype=torch.float32\n",
    "                    ).to(device),\n",
    "                    \"labels\": torch.tensor(\n",
    "                        [obj[\"category_id\"] for obj in t], dtype=torch.int64\n",
    "                    ).to(device),\n",
    "                }\n",
    "                for t in targets\n",
    "            ]\n",
    "            for target in targets:\n",
    "                boxes = target[\"boxes\"]\n",
    "                boxes[:, 2] += boxes[:, 0]  # Convert width to x_max\n",
    "                boxes[:, 3] += boxes[:, 1]  # Convert height to y_max\n",
    "                target[\"boxes\"] = boxes\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            for i, output in enumerate(outputs):\n",
    "                pred_boxes = output[\"boxes\"]\n",
    "                pred_scores = output[\"scores\"]\n",
    "                pred_labels = output[\"labels\"]\n",
    "                true_boxes = targets[i][\"boxes\"]\n",
    "                true_labels = targets[i][\"labels\"]\n",
    "\n",
    "                high_conf_indices = pred_scores > threshold\n",
    "                pred_boxes = pred_boxes[high_conf_indices]\n",
    "                pred_labels = pred_labels[high_conf_indices]\n",
    "\n",
    "                image_index = global_image_index + i\n",
    "\n",
    "                if pred_boxes.size(0) == 0 or true_boxes.size(0) == 0:\n",
    "                    report.append(\n",
    "                        f\"Image {image_index}: No valid predictions or ground truths\"\n",
    "                    )\n",
    "                    continue\n",
    "\n",
    "                iou = box_iou(pred_boxes, true_boxes)\n",
    "                max_iou_indices = iou.argmax(dim=1)\n",
    "                matched_true_labels = true_labels[max_iou_indices]\n",
    "\n",
    "                correct_predictions += (pred_labels == matched_true_labels).sum().item()\n",
    "                total_predictions += len(pred_labels)\n",
    "\n",
    "                mean_iou = iou.max(dim=1)[0].mean().item()\n",
    "                iou_list.append(mean_iou)\n",
    "\n",
    "                accuracy = (\n",
    "                    correct_predictions / total_predictions if total_predictions else 0\n",
    "                )\n",
    "                report.append(\n",
    "                    f\"Image {image_index}: IoU = {mean_iou:.4f}, Accuracy = {accuracy:.4f}\"\n",
    "                )\n",
    "\n",
    "                # Plotting only the first 9 images\n",
    "                if images_plotted < num_images:\n",
    "                    img = images[i].cpu().permute(1, 2, 0)\n",
    "                    ax = axes[images_plotted]\n",
    "\n",
    "                    ax.imshow(img)\n",
    "                    for j in range(len(pred_boxes)):\n",
    "                        bbox = pred_boxes[j].cpu().numpy()\n",
    "                        score = pred_scores[high_conf_indices][j].cpu().item()\n",
    "                        label = pred_labels[j].cpu().item()\n",
    "\n",
    "                        if score > threshold:\n",
    "                            rect = patches.Rectangle(\n",
    "                                (bbox[0], bbox[1]),\n",
    "                                bbox[2] - bbox[0],\n",
    "                                bbox[3] - bbox[1],\n",
    "                                linewidth=2,\n",
    "                                edgecolor=\"r\",\n",
    "                                facecolor=\"none\",\n",
    "                            )\n",
    "                            ax.add_patch(rect)\n",
    "                            ax.text(\n",
    "                                bbox[0],\n",
    "                                bbox[1],\n",
    "                                f\"{label}: {score:.2f}\",\n",
    "                                color=\"white\",\n",
    "                                fontsize=8,\n",
    "                                bbox=dict(facecolor=\"red\", alpha=0.5),\n",
    "                            )\n",
    "                    ax.axis(\"off\")\n",
    "                    images_plotted += 1\n",
    "\n",
    "            global_image_index += len(images)\n",
    "\n",
    "    overall_iou = sum(iou_list) / len(iou_list) if iou_list else 0\n",
    "    overall_accuracy = (\n",
    "        correct_predictions / total_predictions if total_predictions else 0\n",
    "    )\n",
    "\n",
    "    pred_boxes_imgs = \"prediction_grid.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(pred_boxes_imgs)\n",
    "    plt.close()\n",
    "\n",
    "    train_image_base64 = image_to_base64(pred_boxes_imgs)\n",
    "\n",
    "    report_text = \"\\n\".join(report)\n",
    "    overall_report = dedent(\n",
    "        f\"\"\"\n",
    "    Overall Metrics on predictions with confidence threshold {threshold}:\n",
    "    ----------------\n",
    "    Mean IoU: {overall_iou:.4f}\n",
    "    Mean Accuracy: {overall_accuracy:.4f}\n",
    "\n",
    "    Per-Image Metrics:\n",
    "    ------------------\n",
    "    {report_text}\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    ctx = current_context()\n",
    "    deck = Deck(\"Evaluation Results\")\n",
    "    html_report = dedent(\n",
    "        f\"\"\"\n",
    "    <div style=\"font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "       <h2 style=\"color: #2C3E50;\">Predicted Bounding Boxes</h2>\n",
    "        <img src=\"data:image/png;base64,{train_image_base64}\" width=\"600\">\n",
    "    </div>               \n",
    "    <div style=\"font-family: Arial, sans-serif; line-height: 1.6;\">\n",
    "        <h2 style=\"color: #2C3E50;\">Evaluation Report</h2>\n",
    "        <pre>{overall_report}</pre>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    )\n",
    "    deck.append(html_report)\n",
    "    ctx.decks.insert(0, deck)\n",
    "\n",
    "    return overall_report\n",
    "\n",
    "\n",
    "# %% ------------------------------\n",
    "# upload model to hub - task\n",
    "# --------------------------------\n",
    "@task(\n",
    "    container_image=container_image,\n",
    "    requests=Resources(cpu=\"2\", mem=\"2Gi\"),\n",
    "    secret_requests=[Secret(group=None, key=\"hf_token\")],\n",
    ")\n",
    "def upload_model_to_hub(model: torch.nn.Module, repo_name: str) -> str:\n",
    "    from huggingface_hub import HfApi\n",
    "\n",
    "    # Get the Flyte context and define the model path\n",
    "    ctx = current_context()\n",
    "    model_path = \"best_model.pth\"  # Save the model locally as \"best_model.pth\"\n",
    "\n",
    "    # Save the model's state dictionary\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    # Set Hugging Face token from local environment or Flyte secrets\n",
    "    hf_token = os.getenv(\"HF_TOKEN\")\n",
    "    if hf_token is None:\n",
    "        # If HF_TOKEN is not found, attempt to get it from the Flyte secrets\n",
    "        hf_token = ctx.secrets.get(key=\"hf_token\")\n",
    "        print(\"Using Hugging Face token from Flyte secrets.\")\n",
    "    else:\n",
    "        print(\"Using Hugging Face token from environment variable.\")\n",
    "\n",
    "    # Create a new repository (if it doesn't exist) on Hugging Face Hub\n",
    "    api = HfApi()\n",
    "    api.create_repo(repo_name, token=hf_token, exist_ok=True)\n",
    "\n",
    "    # Upload the model to the Hugging Face repository\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=model_path,  # Path to the local file\n",
    "        path_in_repo=\"pytorch_model.bin\",  # Destination path in the repo\n",
    "        repo_id=repo_name,\n",
    "        commit_message=\"Upload Faster R-CNN model\",\n",
    "        token=hf_token,\n",
    "    )\n",
    "\n",
    "    return f\"Model uploaded to Hugging Face Hub: https://huggingface.co/{repo_name}\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9f0c8c",
   "metadata": {},
   "source": [
    "> **üí° Note:**  \n",
    "> In more complex ML workflows, **data pipelines** are often separate from **model training pipelines**.  \n",
    "> For simplicity, we'll combine them into a single workflow in this example.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7e7ef0",
   "metadata": {},
   "source": [
    "## Run Model Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a1aa63",
   "metadata": {},
   "source": [
    "Lets pull down our same dataset locally to run the model on examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d440fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets call our downlaod dataset from earlier locally\n",
    "!union run tasks/data.py download_hf_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee211033",
   "metadata": {},
   "source": [
    "We will pull down the latest Model Artifact from Union and save it locally. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc90613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from union import Artifact, UnionRemote\n",
    "from flytekit.types.file import FlyteFile\n",
    "import torch\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Download & save the fine-tuned model from Union Artifacts\n",
    "# --------------------------------------------------\n",
    "FRCCNFineTunedModel = Artifact(name=\"frccn_fine_tuned_model\")\n",
    "\n",
    "query = FRCCNFineTunedModel.query(\n",
    "    project=\"default\",\n",
    "    domain=\"development\",\n",
    "    # version=\"anmrqcq8pfbnlp42j2vp/n3/0/o0\"  # Optional: specify version. Will download the latest version if not specified\n",
    ")\n",
    "remote = UnionRemote()\n",
    "artifact = remote.get_artifact(query=query)\n",
    "model_file: FlyteFile = artifact.get(as_type=FlyteFile)\n",
    "model = torch.load(model_file.download(), map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "# We'll also load the model for use\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4979977",
   "metadata": {},
   "source": [
    "Let's run the model locally on an example image and draw the bounding boxes on the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84040cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "from flytekit.types.file import FlyteFile\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from torchvision.transforms import functional as F\n",
    "from union import Artifact, UnionRemote\n",
    "from io import BytesIO\n",
    "\n",
    "# Define labels map\n",
    "labels_map = {1: \"Union Sticker\", 2: \"Flyte Sticker\"}\n",
    "\n",
    "# Check and set the available device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "\n",
    "font_url = \"https://github.com/google/fonts/raw/refs/heads/main/apache/ultra/Ultra-Regular.ttf\"\n",
    "response = requests.get(font_url)\n",
    "font = ImageFont.truetype(BytesIO(response.content), size=20)\n",
    "\n",
    "\n",
    "# Function to draw bounding boxes\n",
    "def draw_boxes(image, boxes, labels, scores, labels_map):\n",
    "    draw = ImageDraw.Draw(image, 'RGBA')\n",
    "    # font = ImageFont.truetype(urlopen(truetype_url), size=20)\n",
    "    # font = ImageFont.load_default() # default font in pil\n",
    "\n",
    "\n",
    "    colors = {\n",
    "        0: (255, 173, 10, 200),  # Class 0 color (e.g., blue)\n",
    "        1: (28, 140, 252, 200),  # Class 1 color (e.g., orange)\n",
    "    }\n",
    "    colors_fill = {\n",
    "        0: (255, 173, 10, 100),  # Class 0 fill color (e.g., bluea)\n",
    "        1: (28, 140, 252, 100),  # Class 1 fill color (e.g., orangea)\n",
    "    }\n",
    "\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        if score > 0.6: # adjust threshold as needed\n",
    "          color = colors.get(label, (0, 255, 0, 200))\n",
    "          fill_color = colors_fill.get(label, (0, 255, 0, 100))\n",
    "          draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline=color, width=3)\n",
    "          draw.rectangle([(box[0], box[1]), (box[2], box[3])], fill=fill_color)\n",
    "          label_text = f\"{labels_map[label]}: {score:.2f}\"\n",
    "          text_size = font.getbbox(label_text)\n",
    "          draw.rectangle([(box[0], box[1] - text_size[1]), (box[0] + text_size[0], box[1])], fill=color)\n",
    "          draw.text((box[0], box[1] - text_size[1]), label_text, fill=\"white\", font=font)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "# Load a single test image\n",
    "image_path = '/content/faster-rcnn-object-detection-computer-vision-train-and-deploy/dataset/swag/images/1bd5a6b5-20240916_133544.jpg'\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "image_tensor = F.to_tensor(image).unsqueeze(0).to(device)\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = model(image_tensor)\n",
    "\n",
    "# Get the boxes, labels, and scores\n",
    "boxes = outputs[0]['boxes'].cpu().numpy()\n",
    "labels = outputs[0]['labels'].cpu().numpy()\n",
    "scores = outputs[0]['scores'].cpu().numpy()\n",
    "\n",
    "# Define labels map\n",
    "labels_map = {0: \"Background\", 1: \"Union Sticker\", 2: \"Flyte Sticker\"}\n",
    "\n",
    "# Draw the boxes on the image\n",
    "image_with_boxes = draw_boxes(image, boxes, labels, scores, labels_map)\n",
    "\n",
    "# Display the image\n",
    "image_with_boxes.show()\n",
    "\n",
    "# Save the image\n",
    "image_with_boxes.save('output_image.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c90f64",
   "metadata": {},
   "source": [
    "We can use the same draw bounding boxes function to loop over the frame in a video.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ac2bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import requests\n",
    "import torch\n",
    "from flytekit.types.file import FlyteFile\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from torchvision.transforms import functional as F\n",
    "from union import Artifact, UnionRemote\n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "# ------------------------------------\n",
    "# create video writer\n",
    "# ------------------------------------\n",
    "\n",
    "# Video path and properties\n",
    "video_path = \"dataset/swag/videos/union_sticker_video.mp4\"\n",
    "video = cv2.VideoCapture(video_path)\n",
    "width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "frames_per_second = video.get(cv2.CAP_PROP_FPS)\n",
    "num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "\n",
    "# Initialize video writer\n",
    "video_writer = cv2.VideoWriter(\n",
    "    \"object_detection_video.mp4\",\n",
    "    cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
    "    fps=float(frames_per_second),\n",
    "    frameSize=(width, height),\n",
    "    isColor=True,\n",
    ")\n",
    "\n",
    "\n",
    "def run_inference_video(video, model, device, labels_map):\n",
    "    while True:\n",
    "        hasFrame, frame = video.read()\n",
    "        if not hasFrame:\n",
    "            break\n",
    "\n",
    "        # Convert frame to PIL image\n",
    "        image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        image_tensor = F.to_tensor(image).unsqueeze(0).to(device)\n",
    "\n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            outputs = model(image_tensor)\n",
    "\n",
    "        # Get the boxes, labels, and scores\n",
    "        boxes = outputs[0][\"boxes\"].cpu().numpy()\n",
    "        labels = outputs[0][\"labels\"].cpu().numpy()\n",
    "        scores = outputs[0][\"scores\"].cpu().numpy()\n",
    "\n",
    "        # Draw the boxes on the image\n",
    "        image_with_boxes = draw_boxes(image, boxes, labels, scores, labels_map)\n",
    "\n",
    "        # Convert back to OpenCV image format\n",
    "        result_frame = cv2.cvtColor(np.array(image_with_boxes), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        yield result_frame\n",
    "\n",
    "\n",
    "# Run inference and write video\n",
    "for frame in run_inference_video(video, model, device, labels_map):\n",
    "    video_writer.write(frame)\n",
    "\n",
    "# Release resources\n",
    "video.release()\n",
    "video_writer.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200eb455",
   "metadata": {},
   "source": [
    "This example below shows how you could run the model on a live video feed. This won't run in the notebook, but you can run it in your local environment. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8746ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS WON\"T WORK IN COLAB\n",
    "\n",
    "# Webcam example \n",
    "\n",
    "import torch\n",
    "import cv2\n",
    "import time\n",
    "from torchvision.transforms import functional as F\n",
    "from huggingface_hub import hf_hub_download\n",
    "from union import Artifact, UnionRemote\n",
    "from flytekit.types.file import FlyteFile\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Load the fine-tuned SSD model from Union Artifact\n",
    "# --------------------------------------------------\n",
    "FRCCNFineTunedModel = Artifact(name=\"frccn_fine_tuned_model\")\n",
    "query = FRCCNFineTunedModel.query(\n",
    "    project=\"default\",\n",
    "    domain=\"development\",\n",
    "    # version=\"anmrqcq8pfbnlp42j2vp/n3/0/o0\"  # Optional: specify version\n",
    ")\n",
    "remote = UnionRemote()\n",
    "artifact = remote.get_artifact(query=query)\n",
    "model_file: FlyteFile = artifact.get(as_type=FlyteFile)\n",
    "model = torch.load(model_file.download(), map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Function to process a single frame and draw bounding boxes\n",
    "# --------------------------------------------------\n",
    "def process_frame(frame):\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    image_tensor = F.to_tensor(frame_rgb).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(image_tensor)\n",
    "\n",
    "    boxes = prediction[0]['boxes'].cpu().numpy()\n",
    "    scores = prediction[0]['scores'].cpu().numpy()\n",
    "    labels = prediction[0]['labels'].cpu().numpy()\n",
    "\n",
    "    for i, box in enumerate(boxes):\n",
    "        if scores[i] > 0.5: # Confidence threshold for detection \n",
    "            x_min, y_min, x_max, y_max = box\n",
    "            cv2.rectangle(frame, (int(x_min), int(y_min)), (int(x_max), int(y_max)), (0, 255, 0), 2)\n",
    "            label = f\"Class {labels[i]}: {scores[i]:.2f}\"\n",
    "            cv2.putText(frame, label, (int(x_min), int(y_min) - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "    return frame\n",
    "\n",
    "# --------------------------------------------------\n",
    "# Run feed with frame skipping option for efficiency\n",
    "# --------------------------------------------------\n",
    "def run_video_feed(skip_frames=5):\n",
    "    frame_skip = skip_frames\n",
    "    frame_count = 0\n",
    "    last_processed_frame = None\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video stream.\")\n",
    "        return\n",
    "\n",
    "    prev_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Failed to capture frame.\")\n",
    "            break\n",
    "\n",
    "        current_time = time.time()\n",
    "        fps = 1 / (current_time - prev_time)\n",
    "        prev_time = current_time\n",
    "\n",
    "        if frame_count % frame_skip == 0:\n",
    "            last_processed_frame = process_frame(frame)\n",
    "            fps_text = f\"FPS: {fps:.2f}\"\n",
    "            cv2.putText(last_processed_frame, fps_text, (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "\n",
    "        if last_processed_frame is not None:\n",
    "            cv2.imshow('Object Detection RCNN', last_processed_frame)\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Run the video feed function\n",
    "if __name__ == \"__main__\":\n",
    "    run_video_feed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a72e519",
   "metadata": {},
   "source": [
    "## Serving as an App on Union\n",
    "\n",
    "We can serve our model as an app on Union. This allows us to run the model in a production environment and make it available for use by other applications or users.\n",
    "\n",
    "This example will use Gradio, but we could also use any other web framework like Flask or FastAPI to serve our model as an API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3731cf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üëá Run this command to Serve the trained model in a gradio application\n",
    "!union deploy apps app.py frccn-object-detection-gradio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25e27a8",
   "metadata": {},
   "source": [
    "Just like the training pipeline, the code is added to this notebook for reference with the `%%writefile` magic command to overwrite the files if you want to make changes directly in the notebook. But running the cells below are not required since the code is already in the `workflows/` and `tasks/` folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92194a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "\n",
    "import os\n",
    "from datetime import timedelta\n",
    "from union import Artifact, ImageSpec, Resources\n",
    "from union.app import App, Input, ScalingMetric\n",
    "from flytekit.extras.accelerators import GPUAccelerator, L4\n",
    "\n",
    "# Point to your object detection model artifact\n",
    "FRCCNFineTunedModel = Artifact(name=\"frccn_fine_tuned_model\")\n",
    "\n",
    "image_spec = ImageSpec(\n",
    "    name=\"union-serve-frccn-object-detector\",\n",
    "    packages=[\n",
    "        \"gradio==5.29.0\",\n",
    "        \"torch==2.5.1\",\n",
    "        \"torchvision==0.20.1\",\n",
    "        \"union-runtime>=0.1.18\",\n",
    "        \"opencv-python-headless\",\n",
    "    ],\n",
    "    apt_packages=[\"ffmpeg\", \"libsm6\", \"libxext6\"],\n",
    "    cuda=\"11.8\",\n",
    "    builder=\"union\",\n",
    ")\n",
    "\n",
    "gradio_app = App(\n",
    "    name=\"frccn-object-detection-gradio\",\n",
    "    inputs=[\n",
    "        Input(\n",
    "            name=\"downloaded-model\",\n",
    "            value=FRCCNFineTunedModel.query(),\n",
    "            download=True,\n",
    "        )\n",
    "    ],\n",
    "    container_image=image_spec,\n",
    "    port=8080,\n",
    "    include=[\"./app_main.py\"],  # Include your Streamlit code\n",
    "    args=[\"python\", \"app_main.py\"],\n",
    "    limits=Resources(cpu=\"2\", mem=\"8Gi\", gpu=\"1\"),\n",
    "    requests=Resources(cpu=\"2\", mem=\"8Gi\", gpu=\"1\"),\n",
    "    accelerator=L4,\n",
    "    min_replicas=0,\n",
    "    max_replicas=1,\n",
    "    scaledown_after=timedelta(minutes=2),\n",
    "    scaling_metric=ScalingMetric.Concurrency(2),\n",
    ")\n",
    "\n",
    "# union deploy apps app.py frccn-object-detection-gradio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441bab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app_main.py\n",
    "\n",
    "import time\n",
    "\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "from torchvision.transforms import functional as F\n",
    "\n",
    "# Load model from artifact or fallback path\n",
    "try:\n",
    "    from union_runtime import get_input\n",
    "\n",
    "    model_path = get_input(\"downloaded-model\")\n",
    "except:\n",
    "    model_path = \"frccn_fine_tuned_model.pth\"\n",
    "\n",
    "# Load the model\n",
    "model = torch.load(model_path, map_location=\"cpu\", weights_only=False)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "labels_map = {1: \"union\", 2: \"flyte\"}\n",
    "\n",
    "\n",
    "def detect_objects(frame: np.ndarray) -> np.ndarray:\n",
    "    start = time.time()\n",
    "\n",
    "    pil_img = Image.fromarray(frame).convert(\"RGB\").resize((320, 240))\n",
    "    img_tensor = F.to_tensor(pil_img).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img_tensor)\n",
    "\n",
    "    boxes = outputs[0][\"boxes\"].cpu()\n",
    "    scores = outputs[0][\"scores\"].cpu()\n",
    "    labels = outputs[0][\"labels\"].cpu()\n",
    "\n",
    "    threshold = 0.5\n",
    "    selected = scores > threshold\n",
    "    boxes = boxes[selected]\n",
    "    scores = scores[selected]\n",
    "    labels = labels[selected]\n",
    "\n",
    "    draw = ImageDraw.Draw(pil_img)\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        x1, y1, x2, y2 = box.tolist()\n",
    "        draw.rectangle([x1, y1, x2, y2], outline=\"green\", width=3)\n",
    "        draw.text(\n",
    "            (x1, y1),\n",
    "            f\"{labels_map.get(label.item(), label.item())}: {score:.2f}\",\n",
    "            fill=\"white\",\n",
    "        )\n",
    "\n",
    "    # Overlay inference time and device info\n",
    "    end = time.time()\n",
    "    inference_time = (end - start) * 1000  # ms\n",
    "    debug_text = f\"{device.type.upper()} | {inference_time:.1f} ms\"\n",
    "    draw.rectangle([0, 0, 200, 20], fill=(0, 0, 0, 128))  # semi-transparent background\n",
    "    draw.text((5, 2), debug_text, fill=\"white\")\n",
    "\n",
    "    return np.array(pil_img)\n",
    "\n",
    "\n",
    "# Create Gradio app with upload option\n",
    "demo = gr.Interface(\n",
    "    fn=detect_objects,\n",
    "    inputs=gr.Image(type=\"numpy\", label=\"Upload Image\"),\n",
    "    outputs=gr.Image(type=\"numpy\", label=\"Detection Result\"),\n",
    "    title=\"Union Faster RCNN Object Detection\",\n",
    "    description=\"Upload an image to run Faster RCNN object detection.\",\n",
    ")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(server_name=\"0.0.0.0\", server_port=8080)\n",
    "\n",
    "# union deploy apps app.py frccn-object-detection-gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdf5e5e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
